# BPD-DSRL
# BPD-DSRL

Code and supplementary material for AAAI 2025 paper.

## Checkpoints Available Here!

We provide supervised fine-tuning checkpoints (including cold-start policies and reward models) as well as reinforcement learning checkpoints to facilitate reimplementation and further experimentation.



**BPD-DSRL checkpoints are** [here](https://qubstudentcloud-my.sharepoint.com/my?id=%2Fpersonal%2F40414335_ads_qub_ac_uk%2FDocuments%2FBPD-DSRL-RL-Checkpoints%2FBPD-DSRL-CKPT-SQUAD2&ga=1)



### Evaluating BPD-DSRL

1. Download the checkpoints from the provided [link](https://qubstudentcloud-my.sharepoint.com/my?id=%2Fpersonal%2F40414335_ads_qub_ac_uk%2FDocuments%2FBPD-DSRL-RL-Checkpoints%2FBPD-DSRL-CKPT-SQUAD2&ga=1)

2. To generate outputs using our model checkpoints, run the following command:

   ```bash
   python Eval/generation.py \
       --model_path PATH_TO_YOUR_MODEL \
       --n_test_dataset SELECTED_BENCHMARK_NAME \
       --output_file_name OUTPUT_FILE_NAME
   ```

3. To evaluate the generated outputs using the Bleu-based metrics in our main experiments, run the following command:

   ```bash
   python Eval/main_experiments.py \
       --output_file_path OUTPUT_FILE_PATH
   ```

**NOTE:** **We include all the test results generated by our released reinforcement learning checkpoints in Eval/Test_Results/BPD_DSRL**

## Training From Sratch

### Step 1: Supervised Fine-tuning (SFT)

We first use SFT to initalize the polciy, and learn the outcome reward model (question fluency and answerability).



Execute the following command with our default parameter settings. 

**Policy Warm up (change the <u>*task*</u> varaible for different benchmarks)** 

```bash
source SFT/run_policy.sh
```

**Outcome Reward Model Learning (change the <u>*task*</u> varaible for different benchmarks)** 

```bash
source SFT/run_reward.sh
```

### Step 2: Reinforcement Learning (RL)

Using the model checkpoints from SFT, we further use RL to fine-tune the policy.

